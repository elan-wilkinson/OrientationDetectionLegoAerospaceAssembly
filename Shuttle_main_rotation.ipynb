{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6gPDcuqnpLX",
        "outputId": "4ba017d4-17a6-4ef1-bc12-dc20f110a838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0E4rxzBpiS1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "class QuaternionDataset(Dataset):\n",
        "    def __init__(self, image_folder, label_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.label_folder = label_folder\n",
        "        self.image_files = sorted(os.listdir(image_folder))\n",
        "        self.label_files = sorted(os.listdir(label_folder))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels\n",
        "\n",
        "\n",
        "        lbl_path = os.path.join(self.label_folder, self.label_files[idx])\n",
        "        with open(lbl_path, 'r') as file:\n",
        "            label = torch.tensor([float(x) for x in file.readline().strip().split()], dtype=torch.float32)\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "image_folder = \"/content/drive/MyDrive/Shuttle_main_cropped/images\"\n",
        "label_folder = \"/content/drive/MyDrive/Shuttle_main_cropped/quaternion_labels\"\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "dataset = QuaternionDataset(image_folder, label_folder, transform=transform)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2o2vqkkIuz3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d194d980-c4c4-4fdb-847a-70587e726ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([32, 3, 224, 224])\n",
            "Label batch shape: torch.Size([32, 4])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "for images, labels in dataloader:\n",
        "    print(f\"Image batch shape: {images.shape}\")\n",
        "    print(f\"Label batch shape: {labels.shape}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H1-56R-tZ0O",
        "outputId": "6ad2652c-6624-4aa8-c91a-3a225e8af653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class QuaternionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuaternionModel, self).__init__()\n",
        "        self.base_model = models.resnet50(pretrained=True)\n",
        "        self.base_model.fc = nn.Sequential(\n",
        "            nn.Linear(self.base_model.fc.in_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_model(x)\n",
        "\n",
        "\n",
        "model = QuaternionModel()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tizTx9UYwPRR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "4884bc60-a026-4feb-daff-7cd2cb60ca37"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "optimizer got an empty parameter list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d4800a95346e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using device: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.base_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: requires_grad={param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2FWdIIsEgUe",
        "outputId": "7877a4be-ac01-495e-afaf-060f290d8d18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.conv1.weight: requires_grad=False\n",
            "base_model.bn1.weight: requires_grad=False\n",
            "base_model.bn1.bias: requires_grad=False\n",
            "base_model.layer1.0.conv1.weight: requires_grad=False\n",
            "base_model.layer1.0.bn1.weight: requires_grad=False\n",
            "base_model.layer1.0.bn1.bias: requires_grad=False\n",
            "base_model.layer1.0.conv2.weight: requires_grad=False\n",
            "base_model.layer1.0.bn2.weight: requires_grad=False\n",
            "base_model.layer1.0.bn2.bias: requires_grad=False\n",
            "base_model.layer1.0.conv3.weight: requires_grad=False\n",
            "base_model.layer1.0.bn3.weight: requires_grad=False\n",
            "base_model.layer1.0.bn3.bias: requires_grad=False\n",
            "base_model.layer1.0.downsample.0.weight: requires_grad=False\n",
            "base_model.layer1.0.downsample.1.weight: requires_grad=False\n",
            "base_model.layer1.0.downsample.1.bias: requires_grad=False\n",
            "base_model.layer1.1.conv1.weight: requires_grad=False\n",
            "base_model.layer1.1.bn1.weight: requires_grad=False\n",
            "base_model.layer1.1.bn1.bias: requires_grad=False\n",
            "base_model.layer1.1.conv2.weight: requires_grad=False\n",
            "base_model.layer1.1.bn2.weight: requires_grad=False\n",
            "base_model.layer1.1.bn2.bias: requires_grad=False\n",
            "base_model.layer1.1.conv3.weight: requires_grad=False\n",
            "base_model.layer1.1.bn3.weight: requires_grad=False\n",
            "base_model.layer1.1.bn3.bias: requires_grad=False\n",
            "base_model.layer1.2.conv1.weight: requires_grad=False\n",
            "base_model.layer1.2.bn1.weight: requires_grad=False\n",
            "base_model.layer1.2.bn1.bias: requires_grad=False\n",
            "base_model.layer1.2.conv2.weight: requires_grad=False\n",
            "base_model.layer1.2.bn2.weight: requires_grad=False\n",
            "base_model.layer1.2.bn2.bias: requires_grad=False\n",
            "base_model.layer1.2.conv3.weight: requires_grad=False\n",
            "base_model.layer1.2.bn3.weight: requires_grad=False\n",
            "base_model.layer1.2.bn3.bias: requires_grad=False\n",
            "base_model.layer2.0.conv1.weight: requires_grad=False\n",
            "base_model.layer2.0.bn1.weight: requires_grad=False\n",
            "base_model.layer2.0.bn1.bias: requires_grad=False\n",
            "base_model.layer2.0.conv2.weight: requires_grad=False\n",
            "base_model.layer2.0.bn2.weight: requires_grad=False\n",
            "base_model.layer2.0.bn2.bias: requires_grad=False\n",
            "base_model.layer2.0.conv3.weight: requires_grad=False\n",
            "base_model.layer2.0.bn3.weight: requires_grad=False\n",
            "base_model.layer2.0.bn3.bias: requires_grad=False\n",
            "base_model.layer2.0.downsample.0.weight: requires_grad=False\n",
            "base_model.layer2.0.downsample.1.weight: requires_grad=False\n",
            "base_model.layer2.0.downsample.1.bias: requires_grad=False\n",
            "base_model.layer2.1.conv1.weight: requires_grad=False\n",
            "base_model.layer2.1.bn1.weight: requires_grad=False\n",
            "base_model.layer2.1.bn1.bias: requires_grad=False\n",
            "base_model.layer2.1.conv2.weight: requires_grad=False\n",
            "base_model.layer2.1.bn2.weight: requires_grad=False\n",
            "base_model.layer2.1.bn2.bias: requires_grad=False\n",
            "base_model.layer2.1.conv3.weight: requires_grad=False\n",
            "base_model.layer2.1.bn3.weight: requires_grad=False\n",
            "base_model.layer2.1.bn3.bias: requires_grad=False\n",
            "base_model.layer2.2.conv1.weight: requires_grad=False\n",
            "base_model.layer2.2.bn1.weight: requires_grad=False\n",
            "base_model.layer2.2.bn1.bias: requires_grad=False\n",
            "base_model.layer2.2.conv2.weight: requires_grad=False\n",
            "base_model.layer2.2.bn2.weight: requires_grad=False\n",
            "base_model.layer2.2.bn2.bias: requires_grad=False\n",
            "base_model.layer2.2.conv3.weight: requires_grad=False\n",
            "base_model.layer2.2.bn3.weight: requires_grad=False\n",
            "base_model.layer2.2.bn3.bias: requires_grad=False\n",
            "base_model.layer2.3.conv1.weight: requires_grad=False\n",
            "base_model.layer2.3.bn1.weight: requires_grad=False\n",
            "base_model.layer2.3.bn1.bias: requires_grad=False\n",
            "base_model.layer2.3.conv2.weight: requires_grad=False\n",
            "base_model.layer2.3.bn2.weight: requires_grad=False\n",
            "base_model.layer2.3.bn2.bias: requires_grad=False\n",
            "base_model.layer2.3.conv3.weight: requires_grad=False\n",
            "base_model.layer2.3.bn3.weight: requires_grad=False\n",
            "base_model.layer2.3.bn3.bias: requires_grad=False\n",
            "base_model.layer3.0.conv1.weight: requires_grad=False\n",
            "base_model.layer3.0.bn1.weight: requires_grad=False\n",
            "base_model.layer3.0.bn1.bias: requires_grad=False\n",
            "base_model.layer3.0.conv2.weight: requires_grad=False\n",
            "base_model.layer3.0.bn2.weight: requires_grad=False\n",
            "base_model.layer3.0.bn2.bias: requires_grad=False\n",
            "base_model.layer3.0.conv3.weight: requires_grad=False\n",
            "base_model.layer3.0.bn3.weight: requires_grad=False\n",
            "base_model.layer3.0.bn3.bias: requires_grad=False\n",
            "base_model.layer3.0.downsample.0.weight: requires_grad=False\n",
            "base_model.layer3.0.downsample.1.weight: requires_grad=False\n",
            "base_model.layer3.0.downsample.1.bias: requires_grad=False\n",
            "base_model.layer3.1.conv1.weight: requires_grad=False\n",
            "base_model.layer3.1.bn1.weight: requires_grad=False\n",
            "base_model.layer3.1.bn1.bias: requires_grad=False\n",
            "base_model.layer3.1.conv2.weight: requires_grad=False\n",
            "base_model.layer3.1.bn2.weight: requires_grad=False\n",
            "base_model.layer3.1.bn2.bias: requires_grad=False\n",
            "base_model.layer3.1.conv3.weight: requires_grad=False\n",
            "base_model.layer3.1.bn3.weight: requires_grad=False\n",
            "base_model.layer3.1.bn3.bias: requires_grad=False\n",
            "base_model.layer3.2.conv1.weight: requires_grad=False\n",
            "base_model.layer3.2.bn1.weight: requires_grad=False\n",
            "base_model.layer3.2.bn1.bias: requires_grad=False\n",
            "base_model.layer3.2.conv2.weight: requires_grad=False\n",
            "base_model.layer3.2.bn2.weight: requires_grad=False\n",
            "base_model.layer3.2.bn2.bias: requires_grad=False\n",
            "base_model.layer3.2.conv3.weight: requires_grad=False\n",
            "base_model.layer3.2.bn3.weight: requires_grad=False\n",
            "base_model.layer3.2.bn3.bias: requires_grad=False\n",
            "base_model.layer3.3.conv1.weight: requires_grad=False\n",
            "base_model.layer3.3.bn1.weight: requires_grad=False\n",
            "base_model.layer3.3.bn1.bias: requires_grad=False\n",
            "base_model.layer3.3.conv2.weight: requires_grad=False\n",
            "base_model.layer3.3.bn2.weight: requires_grad=False\n",
            "base_model.layer3.3.bn2.bias: requires_grad=False\n",
            "base_model.layer3.3.conv3.weight: requires_grad=False\n",
            "base_model.layer3.3.bn3.weight: requires_grad=False\n",
            "base_model.layer3.3.bn3.bias: requires_grad=False\n",
            "base_model.layer3.4.conv1.weight: requires_grad=False\n",
            "base_model.layer3.4.bn1.weight: requires_grad=False\n",
            "base_model.layer3.4.bn1.bias: requires_grad=False\n",
            "base_model.layer3.4.conv2.weight: requires_grad=False\n",
            "base_model.layer3.4.bn2.weight: requires_grad=False\n",
            "base_model.layer3.4.bn2.bias: requires_grad=False\n",
            "base_model.layer3.4.conv3.weight: requires_grad=False\n",
            "base_model.layer3.4.bn3.weight: requires_grad=False\n",
            "base_model.layer3.4.bn3.bias: requires_grad=False\n",
            "base_model.layer3.5.conv1.weight: requires_grad=False\n",
            "base_model.layer3.5.bn1.weight: requires_grad=False\n",
            "base_model.layer3.5.bn1.bias: requires_grad=False\n",
            "base_model.layer3.5.conv2.weight: requires_grad=False\n",
            "base_model.layer3.5.bn2.weight: requires_grad=False\n",
            "base_model.layer3.5.bn2.bias: requires_grad=False\n",
            "base_model.layer3.5.conv3.weight: requires_grad=False\n",
            "base_model.layer3.5.bn3.weight: requires_grad=False\n",
            "base_model.layer3.5.bn3.bias: requires_grad=False\n",
            "base_model.layer4.0.conv1.weight: requires_grad=False\n",
            "base_model.layer4.0.bn1.weight: requires_grad=False\n",
            "base_model.layer4.0.bn1.bias: requires_grad=False\n",
            "base_model.layer4.0.conv2.weight: requires_grad=False\n",
            "base_model.layer4.0.bn2.weight: requires_grad=False\n",
            "base_model.layer4.0.bn2.bias: requires_grad=False\n",
            "base_model.layer4.0.conv3.weight: requires_grad=False\n",
            "base_model.layer4.0.bn3.weight: requires_grad=False\n",
            "base_model.layer4.0.bn3.bias: requires_grad=False\n",
            "base_model.layer4.0.downsample.0.weight: requires_grad=False\n",
            "base_model.layer4.0.downsample.1.weight: requires_grad=False\n",
            "base_model.layer4.0.downsample.1.bias: requires_grad=False\n",
            "base_model.layer4.1.conv1.weight: requires_grad=False\n",
            "base_model.layer4.1.bn1.weight: requires_grad=False\n",
            "base_model.layer4.1.bn1.bias: requires_grad=False\n",
            "base_model.layer4.1.conv2.weight: requires_grad=False\n",
            "base_model.layer4.1.bn2.weight: requires_grad=False\n",
            "base_model.layer4.1.bn2.bias: requires_grad=False\n",
            "base_model.layer4.1.conv3.weight: requires_grad=False\n",
            "base_model.layer4.1.bn3.weight: requires_grad=False\n",
            "base_model.layer4.1.bn3.bias: requires_grad=False\n",
            "base_model.layer4.2.conv1.weight: requires_grad=False\n",
            "base_model.layer4.2.bn1.weight: requires_grad=False\n",
            "base_model.layer4.2.bn1.bias: requires_grad=False\n",
            "base_model.layer4.2.conv2.weight: requires_grad=False\n",
            "base_model.layer4.2.bn2.weight: requires_grad=False\n",
            "base_model.layer4.2.bn2.bias: requires_grad=False\n",
            "base_model.layer4.2.conv3.weight: requires_grad=False\n",
            "base_model.layer4.2.bn3.weight: requires_grad=False\n",
            "base_model.layer4.2.bn3.bias: requires_grad=False\n",
            "base_model.fc.0.weight: requires_grad=True\n",
            "base_model.fc.0.bias: requires_grad=True\n",
            "base_model.fc.2.weight: requires_grad=True\n",
            "base_model.fc.2.bias: requires_grad=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j55mNYbsxIzz",
        "outputId": "2956c1ba-f40e-4b40-9502-1fc72009fe63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 626/626 [16:10<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 626/626 [01:26<00:00,  7.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.0311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 626/626 [01:26<00:00,  7.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.0255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 626/626 [01:25<00:00,  7.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.0226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 626/626 [01:25<00:00,  7.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.0210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 626/626 [01:25<00:00,  7.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.0197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 626/626 [01:25<00:00,  7.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.0192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 626/626 [01:25<00:00,  7.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.0178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 626/626 [01:24<00:00,  7.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.0176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 626/626 [01:24<00:00,  7.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.0167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', dynamic_ncols=True)):\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "\n",
        "test_image_path = \"/content/sequence.0.png\"\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "image = Image.open(test_image_path).convert(\"RGB\")\n",
        "image = transform(image)\n",
        "image = image.unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "heEcGsHnMfDP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = model.to(device)\n",
        "image = image.to(device)\n"
      ],
      "metadata": {
        "id": "a8nw-vivNL9c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "\n",
        "\n",
        "print(f\"Predicted quaternion: {output.squeeze().tolist()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yOxwBWVNUFJ",
        "outputId": "73c2816f-7178-4385-a597-79853ce39a01"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted quaternion: [-0.28884392976760864, -0.2874419689178467, 0.13603827357292175, 0.8917813301086426]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "QnGd4qNAN54A"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)"
      ],
      "metadata": {
        "id": "1gKPrHQ1N8pC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_fine_tune_epochs = 5\n",
        "for epoch in range(num_fine_tune_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=f\"Fine-Tune Epoch {epoch+1}/{num_fine_tune_epochs}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Fine-Tune Epoch [{epoch+1}/{num_fine_tune_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/fine_tuned_quaternion_model.pth\")\n",
        "print(\"Fine-tuned model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr4tE_PiOTT4",
        "outputId": "614e170a-4c06-4c98-c9d4-cadc0a7e7c1e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-Tune Epoch 1/5: 100%|██████████| 626/626 [01:24<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tune Epoch [1/5], Loss: 0.0090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-Tune Epoch 2/5: 100%|██████████| 626/626 [01:24<00:00,  7.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tune Epoch [2/5], Loss: 0.0033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-Tune Epoch 3/5: 100%|██████████| 626/626 [01:24<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tune Epoch [3/5], Loss: 0.0025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-Tune Epoch 4/5: 100%|██████████| 626/626 [01:24<00:00,  7.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tune Epoch [4/5], Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fine-Tune Epoch 5/5: 100%|██████████| 626/626 [01:24<00:00,  7.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tune Epoch [5/5], Loss: 0.0013\n",
            "Fine-tuned model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "image = image.to(device)"
      ],
      "metadata": {
        "id": "8XVnCMhWQ6ZV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "\n",
        "\n",
        "print(f\"Predicted quaternion: {output.squeeze().tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_aZ5SU6Q9Ui",
        "outputId": "49156337-90cf-418b-d857-763e7a800760"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted quaternion: [-0.3518476188182831, -0.20073741674423218, 0.08255627751350403, 0.8791753053665161]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def quaternion_to_euler(q):\n",
        "    \"\"\"\n",
        "    Convert a quaternion to Euler angles (roll, pitch, yaw).\n",
        "    Args:\n",
        "        q: Tensor of shape (batch_size, 4), where each row is (w, x, y, z).\n",
        "    Returns:\n",
        "        euler_angles: Tensor of shape (batch_size, 3), where each row is (roll, pitch, yaw).\n",
        "    \"\"\"\n",
        "    q = q / torch.linalg.norm(q, dim=1, keepdim=True)\n",
        "\n",
        "    w, x, y, z = q[:, 0], q[:, 1], q[:, 2], q[:, 3]\n",
        "\n",
        "    roll = torch.atan2(2 * (y * w + x * z), 1 - 2 * (y**2 + z**2))\n",
        "    pitch = torch.asin(2 * (x * y - z * w))\n",
        "    yaw = torch.atan2(2 * (x * w + y * z), 1 - 2 * (x**2 + y**2))\n",
        "\n",
        "    euler_angles = torch.stack((roll, pitch, yaw), dim=1)\n",
        "    return euler_angles\n"
      ],
      "metadata": {
        "id": "DV9i_YpARyT6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output = model(image)\n",
        "\n",
        "\n",
        "print(f\"Predicted quaternion: {quaternion_to_euler(output)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8qrArhESG0F",
        "outputId": "440d2e67-8fd2-40af-8783-9b0a82bb5a96"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted quaternion: tensor([[-2.5529,  0.6692,  0.3251]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "quaternion = torch.tensor([-0.286032081, -0.196760952, 0.06013644, 0.935870945])\n",
        "quaternion = quaternion.unsqueeze(0)"
      ],
      "metadata": {
        "id": "SNQdDilgSZZl"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quaternion_to_euler(quaternion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmaA5zcGSfuz",
        "outputId": "a252f287-2846-48e6-cbbc-c21766a43f7b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.6538,  0.5372,  0.2412]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}